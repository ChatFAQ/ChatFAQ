title,content,url
Designed to scale in your complex business ecosystem  today,Designed to scale in your complex business ecosystem  today,https://www.chatfaq.io/
Architected to stay in control,"Architected to stay in control
It offers complete management over the service experience, from your customers input, up to the AI generative model and deployment architecture.",https://www.chatfaq.io/
Why is it for you?,"Why is it for you?
Plug-and-play for your business
It integrates seamlessly into your business organization by guaranteeing full compliance with your customer care, IT, security and legal constraints.",https://www.chatfaq.io/
Why it stands out,"Why it stands out
Continuously improved with latest AI innovations
It leverages on-going developments around generative Large Language Models LLM technology such as GPT, LLaMa, Falcon models to provide immediacy and response accuracy.",https://www.chatfaq.io/
What is it?,"What is it?
The most advanced conversational interface
It is blending the traditional chat and search experiences into a unique service design that offers built-in chat, search and stand-alone experiences.",https://www.chatfaq.io/
"ChatFAQ, a new AI assistant paradigm to boost your business","It, a new AI assistant paradigm to boost your business
",https://www.chatfaq.io/
Redefine your customer engagement with a generative-AI assistant,Redefine your customer engagement with a generative-AI assistant,https://www.chatfaq.io/
"Architected to stay in control
","Architected to stay in control
It offers complete management over the service experience, from your customers input, up to the AI generative model and deployment architecture.",https://www.chatfaq.io/
Service management,"Service management
Read, review, measure all customers interactions. Incorporate their feedbacks to extend your knowledge database.",https://www.chatfaq.io/
Tailored AI,"Tailored AI
Feed and train with your content to provide relevant information for your business and eliminate AI hallucinations.",https://www.chatfaq.io/
Cloud provider agnostic,"Cloud provider agnostic
It is prepared for standard cloud deployments so you can run service on the infrastructure of your choice.",https://www.chatfaq.io/
Flexible SDK,"Flexible SDK
Brand and adapt the conversation experience to your business with personalised flows, tone and responses.",https://www.chatfaq.io/
Zero license costs,"Zero license costs
No license trap. It is an open-source solution, meaning your costs only grow as you scale your service infrastructure.",https://www.chatfaq.io/
Service many languages,"Service many languages
Provide relevant answers in the preferred languages of your customers without compromising quality.",https://www.chatfaq.io/
100% secure,"100% secure
It has been designed to guarantee the maximum level of security and evolving worldwide regulation compliance
- GDPR Ready
- No third-party
- No user data leakage
- No black-box effect",https://www.chatfaq.io/
"Transform any FAQ into a chat with
generative-AI today!","Transform any FAQ into a chat with
generative-AI today!
Give your business the proper boost it deserves with It generative-AI assistant",https://www.chatfaq.io/
"ChatFAQ, An open-source conversational search platform as an alternative to commercial GPT solutions.
","""It, An open-source conversational search platform as an alternative to commercial GPT solutions.
""",https://www.chatfaq.io/
"Funded by European Union – NextGenerationEU
","It is Funded by European Union – NextGenerationEU

",https://www.chatfaq.io/
Transforming customer experience,"Transforming customer experience
Experience a new level of customer engagement with the It widget. Designed with flexibility and customer experience at its core, our widget blends seamlessly into your brand, offering a unique way to interact, assist, and engage.
",https://www.chatfaq.io/features/widget
Interactive design,"Interactive design
Our widget employs a user-friendly responsive design that makes it intuitive for customers to interact with, ensuring a smooth and positive experience.",https://www.chatfaq.io/features/widget
Seemless integration,"Seemless integration
Our widget easily embeds into your web site or platform, offering immediate access to enhanced customer service.",https://www.chatfaq.io/features/widget
Multilingual support,"Multilingual support
Our widget can communicate in multiple languages, thereby enabling businesses to serve a global customer base without compromising on service quality.",https://www.chatfaq.io/features/widget
Fully brandable UX/UI,"Fully brandable UX/UI
Create an experience that reflects your brand's uniqueness and caters to your audience's needs. The It widget offers full flexibility over all UI aspects (size, color, fonts, and logo) to align with your brand identity.",https://www.chatfaq.io/features/widget
Trackable and usable conversations,"Trackable and usable conversations
We understand the importance of accessibility and traceability. With the It widget, conversations are logged and accessible anytime, for both customers and administrators. This functionality enables consistent customer service and the ability to track and improve interactions based on past conversations.",https://www.chatfaq.io/features/widget
Packed with advanced capabilities,"Packed with advanced capabilities
It widget comes with many built-in features to give the best experience to your users: real-time response generation, negative and positive user voting flows, conversation history management. Soon, our widget will support standalone page functionality and a search bar module to streamline navigation and provide quicker access to information.",https://www.chatfaq.io/features/widget
Communicate your brand with generative AI models,"Communicate your brand with generative AI models
It leverages Generative Language models like Large Language Models (LLMs) to offer unprecedented capabilities in natural language processing and generation. This allows to deliver a sophisticated chatbot service that comprehends and generates human-like text and empowers brands to deliver personalized and efficient communication at scale.",https://www.chatfaq.io/features/generative-ai
Your Brand Business Domain,"Your Brand Business Domain
By understanding the brand's business domain, the AI model can assist customers with more specific and tailored information, helping them make informed decisions and improving their perception of the brand's expertise and credibility.",https://www.chatfaq.io/features/generative-ai
Meeting your Customer Needs,"Meeting your Customer Needs
Stay ahead of evolving customer demands to enhance the chatbot's functionality over time by identifying new business opportunities or areas for improvement based on customer feedback and interactions that chatFAQ will report to you.",https://www.chatfaq.io/features/generative-ai
Controlling Data privacy,"Controlling Data privacy
Unlike third-party SaaS chatbots, It implements robust data privacy measures, such as data encryption, self-hosting, and anonymization techniques to ensure user questions and company information are kept secure and confidential.",https://www.chatfaq.io/features/generative-ai
Automatic knowledge expansion,"Automatic knowledge expansion
Upload your business content as CSV or PDF files and It will generate utterances to expand your knowledge dataset and improve the AI model accuracy. If you don’t have any existing Frequently Asked Questions, no need to manually create them from scratch. It will infer FAQ automatically and prepare the training dataset covering your business context.",https://www.chatfaq.io/features/generative-ai
Powerful and secured natural language pipeline,"Powerful and secured natural language pipeline
It pipeline is architected on a flexible multi-stage processing that transforms user questions into embedding vectors that are matched by an intent-correlation model. Once the user intent is understood and proper business context has been built, a custom prompt engineering is applied to perform NLG generation with hallucination and adversarial prompt preventions.",https://www.chatfaq.io/features/generative-ai
Best selection of AI models,"Best selection of AI models
Running LLM models is a complex and costly task both on local or cloud deployments. Among the best open-source LLM models, It selects 2 models for development and production purpose in order to achieve the best performance (in number of generated tokens per second) with the minimum hardware footprint (memory and CPU/GPU requirements).",https://www.chatfaq.io/features/generative-ai
Experience a unified workflow with our built-in integrations,"Experience a unified workflow with our built-in integrations
It’s powerful integration capabilities open the door to an elevated customer experience. By seamlessly interconnecting with your existing ecosystem, we enable enhanced functionalities across various touchpoints.",https://www.chatfaq.io/features/integrations
Ease of integration,"Ease of integration
CharFAQ plug-and-play system allows effortlessly integration into your existing framework, while complying with your business's customer care, IT, security and legal requirements.",https://www.chatfaq.io/features/integrations
Versatile compatibility,"Versatile compatibility
It's integration ability ensures that all your front-end and back-end systems work in harmony, creating a seamless customer experience.",https://www.chatfaq.io/features/integrations
Scalability,"Scalability
Our chat interface is designed to scale, fitting perfectly with your growing business needs and easily adapting to changing customer expectations.",https://www.chatfaq.io/features/integrations
ChatFAQ SDK: a foundation for integration,"It SDK: a foundation for integration
With our flexible SDK, you have the tools at your disposal to create a personalized interaction experience. Tailor the conversation flow, adapt the tone, and set up custom responses that align with your brand. Acting as the cornerstone of our integration abilities, the SDK is adaptable across your preferred platforms, ensuring a consistent brand voice and customer experience.",https://www.chatfaq.io/features/integrations
Versatile compatibility,"Versatile compatibility
Our service blends into your existing tools, ensuring seamless integration with your CRM, ERP, CLOUD, and CMS systems. This minimizes disruptions, fosters efficiency, and supports even the most complex architectures. It works harmoniously with a broad range of platforms such as Salesforce, HubSpot, Microsoft Dynamics, ServiceNow, Magento, Shopify, Hybris Commerce , and more, ensuring a streamlined operation across your business ecosystem.",https://www.chatfaq.io/features/integrations
Elevate your user interface,"Elevate your user interface
Enrich your front-end user interface with the integration of It. Our front-end integration capabilities allow for the creation of an engaging and personalized customer experience across various platforms. Apart from our web widgets, you can easily plug internal communication tools like Slack and Teams, or external platforms such as WhatsApp and Messenger, It ensures you're connected wherever your customers are.",https://www.chatfaq.io/features/integrations
Contact us,"[Contact Us](https://www.chatfaq.io/contact-us)
Get In Touch With Us
Please provide the requested information below and leave your message in the designated field. We will get back to you as soon as possible.
Thank you for reaching out to us!",https://www.chatfaq.io/contact-us
ChatFAQ news,"[It news](https://www.chatfaq.io/blog)
Check our last events, updates and articles about It ",https://www.chatfaq.io/blog
init > chatfaq-s-documentation,"init > chatfaq-s-documentation: Welcome to the official documentation of It, a comprehensive open-source community-driven platform for creating AI chatbots.

If you are new to this documentation, we recommend that you read the [introduction](https://chatfaq.readthedocs.io/en/latest/introduction.html) page to get an overview of what this documentation has to offer.

The table of contents in the sidebar should let you easily access the documentation for your topic of interest. You can also use the search function in the top-left corner.",https://chatfaq.readthedocs.io/en/latest/index.html#chatfaq-s-documentation
init > get-involved,"init > get-involved: It is an open source project developed by a community of volunteers. The documentation team can always use your feedback and help to improve the tutorials and class reference. If you don’t understand something, or cannot find what you are looking for in the docs, help us make the documentation better by letting us know!

Submit an issue or pull request on the [GitHub repository](https://github.com/It/It/)",https://chatfaq.readthedocs.io/en/latest/index.html#get-involved
introduction > installation,"introduction > installation: The system comprises three main components that you need to install:

It Components: Widget, Backend and SDK

The back-end [install](https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#back-installation) manages the communication between all the components. It also houses the database for storing all the data related to the chatbots, datasets, models, etc…

The SDK [install](https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#sdk-installation) launches a Remote Procedure Call (RPC) server to execute transitions and events from the posted FSM definitions.

The widget [install](https://chatfaq.readthedocs.io/en/latest/introduction.html#:~:text=The%20system%20comprises,with%20the%20bot.) is a JS browser client application from which the user interacts with the bot.",https://chatfaq.readthedocs.io/en/latest/introduction.html#installation
introduction > model-configuration,"introduction > model-configuration: After setting up the components, you will probably want to configure a model that you want to use for your chatbot. Typically the model will be used from the SDK, from a state within its FSM.

Here is an example of a minimum model [configuration](https://chatfaq.readthedocs.io/en/latest/introduction.html#:~:text=After%20setting%20up,model%20(configuration))",https://chatfaq.readthedocs.io/en/latest/introduction.html#model-configuration
introduction > quick-start,introduction > quick-start: Learning [how to use the SDK](https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html#usage) is the only requirement to start building your own chatbots with It.,https://chatfaq.readthedocs.io/en/latest/introduction.html#quick-start
modules > installations > back-installation,"modules > installations > back-installation: This is It’s core component, the orchestrator of ChatGPT. It manages all the widgets and SDks connections, session storage, datasets and models registration, FSM registration, FSM executions (intended only for simple FSMs), etc…
",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#back-installation
modules > installations > back-installation > setting-it-up-locally > prerequisites,"modules > installations > back-installation > setting-it-up-locally > prerequisites: Make sure the next list of packages are installed on your system:

- Python 3.10

- python3.10-dev

- python3.10-distutils

- PostgreSQL

- pgvector

- gdal-bin

- poetry",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#prerequisites
modules > installations > back-installation > installation > local-build > set-up,"modules > installations > back-installation > installation > local-build > set-up: Install project dependencies:

```
poetry install
```

Create a “chatfaq” database in PostgreSQL

```
sudo -u postgres psql -c ""CREATE DATABASE chatfaq""
```

Create a “chatfaq” user in PostgreSQL
```
sudo -u postgres psql -c ""CREATE user chatfaq WITH encrypted password 'chatfaq';""
```

Give the newly created user the necessary privileges

```
sudo -u postgres psql -c ""grant all privileges on database chatfaq to chatfaq;""
```

Apply django migrations

```
poetry run ./manage.py migrate
```

Apply fixtures

```
make apply_fixtures
```

Create a superuser

```
./manage.py createsuperuser --rpc_group 1
```

When creating the superuser notice that we are passing the `--rpc_group 1` flag. This is critical to be able to create an RPC Server with this same user later on.

Now you can access the Django admin panel at http://localhost:8000/back/admin/ and login with the superuser credentials, from there you can CRUD all the relevant models.",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#set-up
modules > installations > back-installation > installation > local-build > run,"modules > installations > back-installation > installation > local-build > run: First of all, create a `.env` file with the needed variables set. You can see an example of those on [.env-template](.env-template) file. Next you can see the explanation of each variable:

`DEBUG`: Set to ""yes"" to enable debug mode

`SECRET_KEY`: Server secret key. This is used to provide cryptographic signing, and should be set to a unique, unpredictable value.

`DATABASE_URL`: Database connection URL. This is the URL that will be used to connect to the database. It should be in the following format: `postgres://USER:PASSWORD@HOST:PORT/NAME`

`BASE_URL`: Base URL of the server. This is the URL that will be used to connect to the server. It should be in the following format: `http://HOST:PORT`

`AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` + `AWS_STORAGE_BUCKET_NAME` + `DO_REGION` + `STORAGES_MODE` + `STORAGE_MAKE_FILES_PUBLIC`: These variables are used to configure the storage backend. If you want to use AWS S3, you should set `STORAGES_MODE` to ""s3"" and set the other variables accordingly. If you want to use Digital Ocean Spaces, you should set `STORAGES_MODE` to ""spaces"" and set the other variables accordingly. If you want to use the local filesystem, you should set `STORAGES_MODE` to ""local"".

`TG_TOKEN`, `WHATSAPP_TOKEN`, `SIGNAL_TOKEN`, `FB_TOKEN`: These variables are used to configure the messaging platforms. You should set the token of the platforms you want to use. If you don't want to use a platform (ie: you are using our [Widget](../widget/README.md) solution), you can leave its token empty.

Run the server

    make run",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#run
modules > installations > back-installation > setting-it-up-locally > docker,modules > installations > back-installation > setting-it-up-locally > docker: Alternatively you can simply run the server using docker.,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#docker
modules > installations > installation > docker > build,modules > installations > installation > docker > build:     docker build -t chatfaq-back .,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#build
modules > installations > installation > docker > build > run,modules > installations > installation > docker > build > run:     docker run -p 8000:8000 chatfaq-back,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id1
modules > installations > usage > useful-endpoints,"modules > installations > usage > useful-endpoints: Admin: [http://localhost:8000/back/admin/](http://localhost:8000/back/admin/)

Auth Token Generation: [http://localhost:8000/back/api/login/](http://localhost:8000/back/api/login/)

Swagger Docs: [http://localhost:8000/back/api/schema/swagger-ui/](http://localhost:8000/back/api/schema/swagger-ui/)

Redoc Docs: [http://localhost:8000/back/api/schema/redoc/](http://localhost:8000/back/api/schema/redoc/)",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#useful-endpoints
modules > installations > sdk-installation,"modules > installations > sdk-installation: For those chatbots with complex Finite State Machine (FSM) behaviours, you will probably want to run them on a separate process, that is what for the SDK is made for. Its primary function is to execute the FSM's computations (transition's conditions and states) by running Remote Procedure Call (RPC) server that listen to the back-end requests.",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#sdk-installation
modules > installations > sdk-installation > prerequisites,"modules > installations > sdk-installation > prerequisites: Make sure the next list of packages are installed on your system:

- Python 3.10
- python3.10-dev
- python3.10-distutils
- poetry",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id2
modules > installations > sdk-installation > pypi,modules > installations > sdk-installation > pypi:     poetry add chatfaq-sdk,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#pypi
modules > installations > sdk-installation > local-build > set-up,"modules > installations > sdk-installation > local-build > set-up: Install project dependencies:

    poetry install",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id5
modules > installations > sdk-installation > local-build > run,"modules > installations > sdk-installation > local-build > run: First of all, create a `.env` file with the needed variables set. You can see an example of those on [.env-template](.env-template) file. Next you can see the explanation of each variable:

`CHATFAQ_RETRIEVAL_HTTP`: The address for the HTTP of the back-end server.

`CHATFAQ_BACKEND_WS`: The address for the WS of the back-end server.

`CHATFAQ_TOKEN`: The token to authenticate with the back-end server. You can retrieve the auth token from the backend server:

`curl -X POST -u username:password http://localhost:8000/back/api/login/`

Run the example:

    make run_example

This will run the example FSM that is located in [./examples/model_example/__init__.py](./examples/model_example/__init__.py) file. You can modify this file to test your own FSMs.",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id6
modules > installations > sdk-installation > docker,modules > installations > sdk-installation > docker: Alternatively you can simply run the server using docker.,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id7
modules > installations > sdk-installation > docker > build,modules > installations > sdk-installation > docker > build:     docker build -t chatfaq-sdk .,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id8
modules > installations > sdk-installation > docker > run,modules > installations > sdk-installation > docker > run:     docker run chatfaq-sdk,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id9
modules > installations > widget-installation,"modules > installations > widget-installation: We built for you a custom front-end solution just so you can talk with your chatbot from the browser using an app you own. Although you can also connect any other message platform as such WhatsApp, Telegram, Signal, Facebook messenger, etc... It supports them all and if it doesn't it can easily be extended to do so.",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#widget-installation
modules > installations > widget-installation > prerequisites,"modules > installations > widget-installation > prerequisites: Make sure the next list of packages are installed on your system:

- npm
- node v19.6.0",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id10
modules > installations > widget-installation > instalaltion > npm,modules > installations > widget-installation > instalaltion > npm:     npm install chatfaq-widget,https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#npm
modules > installations > widget-installation > instalaltion > unpkg,"modules > installations > widget-installation > instalaltion > unpkg:     <script src=""unpkg.com/chatfaq-widget/dist/widget-loader.min.esm""></script>",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#unpkg
modules > installations > widget-installation > instalation > local-build > set up,"modules > installations > widget-installation > instalation > local-build > set up: Install project dependencies:

    npm i",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id13
modules > installations > widget-installation > run,"modules > installations > widget-installation > run: First of all, create a `.env` file with the needed variables set. You can see an example of those on [.env-template](.env-template) file. Next you can see the explanation of each variable:

`CHATFAQ_BACKEND_API`: The address for the HTTP of the back-end server.

`CHATFAQ_BACKEND_WS`:  The address for the WS of the back-end server.

Run the example:

    npm run dev

This will run a node server which will serve an empty webpage with just the Widget integrated on it, if you navigate to http://localhost:3000",https://chatfaq.readthedocs.io/en/latest/modules/installations/index.html#id14
modules > sdk > sdk-documentation,"modules > sdk > sdk-documentation: For those chatbots with complex Finite State Machine (FSM) behaviours, you will probably want to run them on a separate process, that is what for the SDK is made for. Its primary function is to execute the FSM's computations (transition's conditions and states) by running Remote Procedure Call (RPC) server that listen to the back-end requests.",https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html
modules > sdk > sdk-documentation > usage > simple-example,"modules > sdk > sdk-documentation > usage > simple-example: This is just a dummy example that displays the basic usage of the library.

We are going to build the next FSM:

![fsm](../../../../doc/source/_static/images/simple_fsm_diagram.png)

Import basic modules to build your first FMS:

```python
import os
import random
from chatfaq_sdk import ItSDK
from chatfaq_sdk.fsm import FSMDefinition, State, Transition
from chatfaq_sdk.conditions import Condition
from chatfaq_sdk.layers import Text
```

Declare the 3 possible states of our FSM:


```python
def send_greeting(ctx: dict):
    yield Text(""Hello!"")
    yield Text(""How are you?"", allow_feedback=False)

greeting_state = State(name=""Greeting"", events=[send_greeting], initial=True)


def send_answer(ctx: dict):
    last_payload = ctx[""last_mml""][""stack""][0][""payload""]
    yield Text(
        f'My answer to your message: ""{last_payload}"" is: {random.randint(0, 999)}'
    )
    yield Text(f""Tell me more"")

answering_state = State(
    name=""Answering"",
    events=[send_answer],
)


def send_goodbye(ctx: dict):
    yield Text(""Byeeeeeeee!"", allow_feedback=False)

goodbye_state = State(
    name=""Goodbye"",
    events=[send_goodbye],
)

```

Declare the only computable condition for the transitions of our FSM:


```python
def is_saying_goodbye(ctx: dict):
    if ctx[""last_mml""][""stack""][0][""payload""] == ""goodbye"":
        return Condition(1)
    return Condition(0)
```

Now lets glue everything together:

Declare our transitions

```python
any_to_goodbye = Transition(dest=goodbye_state, conditions=[is_saying_goodbye])

greeting_to_answer = Transition(
    source=greeting_state,
    dest=answering_state,
    unless=[is_saying_goodbye],
)
answer_to_answer = Transition(
    source=answering_state, dest=answering_state, unless=[is_saying_goodbye]
)
```

Build the final instance of our FSM:

```python
fsm_definition = FSMDefinition(
    states=[greeting_state, answering_state, goodbye_state],
    transitions=[greeting_to_answer, any_to_goodbye, answer_to_answer],
)
```

Finally, run the RPC server loop with the previously built FSM:

```python
import os

sdk = ItSDK(
    chatfaq_retrieval_http=""http://localhost:8000"",
    chatfaq_ws=""ws://localhost:8000"",
    token=os.getenv(""CHATFAQ_TOKEN""),
    fsm_name=""my_first_fsm"",
    fsm_definition=fsm_definition,
)
sdk.connect()
```
",https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html#simple-example
modules > sdk > sdk-documentation > usage > model-example,"modules > sdk > sdk-documentation > usage > model-example: All of that is great, but where is the large language model capabilities that It offers?

What if we want to build a FSM that makes use of a Language Model?

For that, you first need to [configure your model](../configuration/index.md).

Once you have configured all the components of the model, you will just need to reference the name of your RAG Configuration inside a state of the FSM.

For example, if you have a RAG Configuration named `my_rag_config`, you can use it inside a state like this:

```python
from chatfaq_sdk.fsm import FSMDefinition, State, Transition
from chatfaq_sdk.layers import LMGeneratedText, Text


def send_greeting(ctx: dict):
    yield Text(""How can we help you?"", allow_feedback=False)



def send_answer(ctx: dict):
    last_payload = ctx[""last_mml""][""stack""][0][""payload""]
    yield LMGeneratedText(last_payload, ""my_rag_config"")

greeting_state = State(name=""Greeting"", events=[send_greeting], initial=True)

answering_state = State(
    name=""Answering"",
    events=[send_answer],
)

_to_answer = Transition(
    dest=answering_state,
)

fsm_definition = FSMDefinition(
    states=[greeting_state, answering_state],
    transitions=[_to_answer]
)
```

For the sake of completeness, here is the diagram of this FSM:

![fsm](../../../../doc/source/_static/images/model_fsm_diagram.png)
",https://chatfaq.readthedocs.io/en/latest/modules/sdk/index.html#model-example
modules > widget,"modules > widget: We built for you a custom front-end solution just so you can talk with your chatbot from the browser using an app you own. Although you can also connect any other message platform as such WhatsApp, Telegram, Signal, Facebook messenger, etc... It supports them all and if it doesn't it can easily be extended to do so.
",https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html
modules > widget > usage > js-library,"modules > widget > usage > js-library:
```html
<div id=""chatfaq-widget""></div>

<script>
    import { ChatfaqWidget } from ""chatfaq-widget/dist/widget-loader.esm"";

    const config = {
        element: ""#chatfaq-widget"",
        chatfaqApi: ""http://127.0.0.1:8000"",
        chatfaqWs: ""ws://127.0.0.1:8000"",
        userId: 1234567890,
        fsmDef: ""simple_fsm"",
        title: ""Hello there 👋"",
        subtitle: ""How can we help you?"",
        historyOpened: true,
        maximized: false
    }

    const chatfaqWidget = new ChatfaqWidget(config);

</script>
```

It is also possible to pass the config keys as data attributes to the mounted element as such:

```html
<div
    id=""chatfaq-widget""
    data-chatfaq-api=""http://127.0.0.1:8000""
    data-chatfaq-ws=""ws://127.0.0.1:8000""
    user-id=""1234567890""
    data-fsm-def=""simple_fsm""
    data-title=""Hello there 👋""
    data-subtitle=""How can we help you?""
    history-opened=""true""
    maximized=""false""
></div>
```
If you declare data attributes and a config object and its keys collide, then the config object will have priority.
",https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#js-library
modules > widget > usage > web-component,"modules > widget > usage > web-component: ```html

<script>
    import { ChatfaqWidgetCustomElement } from ""chatfaq-widget/dist/widget-loader.esm"";
    customElements.define(""chatfaq-widget"", ChatfaqWidgetCustomElement)
</script>

<chatfaq-widget
    data-chatfaq-api=""http://127.0.0.1:8000""
    data-chatfaq-ws=""ws://127.0.0.1:8000""
    data-user-id=""1234567890""
    data-fsm-def=""simple_fsm""
    data-title=""Hello there 👋""
    data-subtitle=""How can we help you?""
    data-history-opened=""true""
    data-maximized=""false""
></chatfaq-widget>
```",https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#web-component
modules > widget > usage > widget-params,"modules > widget > usage > widget-params: Next we will explain all the widget's possible parameters:

`element`: string selector or HTMLElement to which the widget will be attached.

`chatfaqApi`: url of the chatfaq-api.

`chatfaqWs`: url of the chatfaq-ws.

`userId`: In case you want to keep track of the user's conversations, you can pass a userId to the widget. This id will be store as a cookie and will be sent to the backend on each request. Later on the widget will be able to retrieve the conversations history of the user.

`fsmDef`: name of the FSM definition to use.

`title`: title which will appear on the header of the chatbot

`subtitle`: subtitle which will appear on the footer of the chatbot

`historyOpened`: if the widget starts with the left menu opened.

`maximized`: if the widget starts maximized.",https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#widget-params
modules > widget > usage > widget-styles,"modules > widget > usage > widget-styles: We made the widget styles hightly customizable by exposing a set of variables that controls greatly the look and feel if it. You can easely overwrite them as shown in the next example:

```html

<script type=""text/javascript"" src=""chatfaq-widget/dist/widget-loader.esm""></script>
<style>
    :root {
        --chatfaq-color-primary-200: red;
        --chatfaq-color-secondary-pink-500: blue;
        --chatfaq-color-tertiary-blue-500: green;
        --chatfaq-color-tertiary-green-500: black;
    }
</style>
```

You can find the full list of variables on [widget/assets/styles/_variables.css](../../../../widget/assets/styles/_variables.css)",https://chatfaq.readthedocs.io/en/latest/modules/widget/index.html#widget-styles
modules > ia-configuration > init,"modules > ia-configuration > init: After setting up the components, you will probably want to configure a model that you want to use for your chatbot. Typically the model will be used from the SDK, from a state within its FSM.
",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html
modules > ia-configuration > the-rag-pipeline,"modules > ia-configuration > the-rag-pipeline: The [RAG (Retrieval-Augmented Generation)](https://arxiv.org/abs/2005.11401) architecture is a workflow that combines a retriever and a generator. The retriever is used to retrieve the most relevant knowledge items from a knowledge base, and the generator is used to generate an answer from the retrieved knowledge items.

In It we choose to follow this pattern. Next we explain how to configure it.

We define 5 main components that are needed to configure a RAG pipeline:

- Knowledge Base
- Retriever
- Prompt
- Generation
- LLM
- RAG

It provide in its fixtures a default configuration for each of these components except for the Knowledge Base and the RAG Config. You can apply the fixtures by simply running the following command from the `back` directory:

```bash
make apply_fixtures
```

We also provide an example of each component here, so you can use it as a reference.

Currently all the relevant data/models can be accessed and modified from the Django admin panel ([http://localhost/back/admin/](http://localhost/back/admin/)) or from the CLI.",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#the-rag-pipeline
modules > ia-configuration > the-rag-pipeline > knowledge-base,"modules > ia-configuration > the-rag-pipeline > knowledge-base: The knowledge base is your source of truth.

It typically starts with a collection of documents (CSVs, PDFs or URLs) that the chatbot will use to answer questions.

Once a knowledge base is created, the system will parse your source document and generate the corresponding knowledge items.

Next we list the different properties that a of knowledge bases has.

- **lang**: The language of the knowledge base. It is used to tokenize the documents.
- **original_csv**: The CSV file.
- **original_pdf**: The PDF file.
- **original_url**: The URL.

<b> CSV parsing options</b>

- **csv_header**: Whether the CSV file has a header or not.
- **title_index_col**: The index of the column that contains the title of the knowledge item.
- **content_index_col**: The index of the column that contains the content of the knowledge item.
- **url_index_col**: The index of the column that contains the URL of the knowledge item.
- **section_index_col**: The index of the column that contains the section of the knowledge item.
- **role_index_col**: The index of the column that contains the role of the knowledge item.
- **page_number_index_col**: The index of the column that contains the page number of the knowledge item.

<b> PDF and URL parsing options</b>

- **strategy**: The strategy to use to parse the PDF files. Can be 'auto', 'fast', 'ocr' or 'high_res'. Default: 'fast'.
- **recursive**: Whether to recursively parse the URLs or not. Default: True.
- **splitter**: The splitter used to split the documents into chunks. It is used to generate the knowledge items. Can be 'sentences', 'words', 'tokens' and 'smart'. Default: 'sentences'.
- **chunk_size**: The number of tokens per chunk. It is used by the splitter to split the documents into chunks. Default: 128.
- **chunk_overlap**: The number of tokens that overlap between two chunks. Default: 16.",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#knowledge-base
modules > ia-configuration > the-rag-pipeline > knowledge-base > recommendations,"modules > ia-configuration > the-rag-pipeline > knowledge-base > recommendations: - The **strategy** to use depends on the time that you want to wait for the parsing process to finish and the quality of the parsing process. The strategies are ordered from fastest to slowest and from worst quality to best quality. The 'fast' strategy is the default one and it is the one that we recommend for most use cases, it only lasts a few seconds and it has a good quality. The 'high_res' strategy is the one with the best quality but it can last several minutes. For more information about the different strategies check [here](https://unstructured-io.github.io/unstructured/bricks/partition.html#partition-pdf).

- The **splitter** that we recommend is the 'sentences' one. It splits the documents into sentences and then it merges the sentences into chunks of approximately 'chunk_size' tokens. This is the best option for most use cases. The 'smart' splitter uses GPT-4 to split the documents into semantic meaningful chunks, this is **VERY EXPENSIVE** and can bump into OpenAI rate limits (will be optimized in the future).

- The **chunk_size** that we recommend is 128. This is the default value and it is the one that we recommend for most use cases, it is a good balance between retrieval quality and information density. If you want to increase the retrieval quality you can decrease this value, but it will decrease the information density of a chunk.

- The **chunk_overlap** is used when splitting with the 'words' or 'tokens' splitters. 16 or 32 is enough for not losing information between chunks.
",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#recommendations
modules > ia-configuration > the-rag-pipeline > knowledge-base > csv structure,"modules > ia-configuration > the-rag-pipeline > knowledge-base > csv structure: An example of a CSV for the Knowledge Base is the following:

| title | content | url | section | role |
| --- | --- | --- | --- | --- |
| Can It integrate with communication tools like Slack and Teams? | Yes, It can integrate with communication tools like Slack and Teams, enhancing your communication capabilities and enabling seamless interactions with your audience. | https://www.chatfaq.io/features/integrations | Features > Integrations | user |
| Can the It Widget be tailored to fit specific brand identities? | Absolutely, the It Widget can be fully branded to reflect your brand's uniqueness, including size, color, fonts, and logo. This ensures it aligns perfectly with your brand identity. | https://www.chatfaq.io/features/widget | Features > Widget | user |
| Does It offer a customized Natural Language Processing (NLP) engine? | Yes, It includes a specialized NLP/NLG engine that enhances the conversational capabilities of chatbots, making them more effective in understanding and responding to user queries. | https://github.com/It/It | GitHub > Documentation | user |
| Does It offer specific enterprise solutions? | Indeed, It is suitable for businesses and can be tailored to meet enterprise needs. It offers features and customization options suitable for businesses of all sizes. | https://github.com/It/It | GitHub > About | user |
| Does the It Widget support multiple languages? | Yes, the It Widget is multilingual, allowing businesses to communicate with a global customer base while maintaining service quality. | https://www.chatfaq.io/features/widget | Features > Widget | user |
| How can I customize the user interface of the It Widget? | The It Widget offers complete flexibility over UI aspects, including size, color, fonts, and logo, to align with your brand's uniqueness and cater to your audience's needs. | https://www.chatfaq.io/features/widget | Features > Widget | user |
| How can I expand my knowledge dataset with It? | You can expand your knowledge dataset with It by uploading your business content as CSV or PDF files. It will automatically generate utterances to enhance your knowledge dataset, improving the accuracy of the AI model. Even if you don't have existing Frequently Asked Questions, It can infer FAQs and prepare a training dataset covering your business context. | https://www.chatfaq.io/features/generative-ai | Features > Generative AI | user |
| ... | ... | ... | ... | ... |",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#csv-structure
modules > ia-configuration > the-rag-pipeline > retriever-config,"modules > ia-configuration > the-rag-pipeline > retriever-config: The retriever is the component that will retrieve the most relevant knowledge items from the knowledge base.

The retriever is configured with the following properties:

- **name**: Just a name for the retriever.
- **model_name**: The name of the retriever model to use. It must be a HuggingFace repo id. Default: 'intfloat/e5-small-v2'.
- **batch_size**: The batch size to use for the retriever. Default: 1.
- **device**: The device to use for the retriever. It can be a CPU or a GPU. Default: 'cpu'.

We recommend setting the **model_name** to one of the [e5 family models](https://huggingface.co/intfloat). The retriever is developed with these models as the base, so it will work better with them. We suggest to use [intfloat/e5-small-v2](https://huggingface.co/intfloat/e5-small-v2) for English and [intfloat/multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) for other languages.

For **batch_size** we recommend using 1 for CPU and for GPU as much as your GPU can handle. For personal use, batch size of 1 is enough, but for production use, you should use a higher batch size and a GPU.

For **device** we recommend using a GPU if you have one available. For personal use it is enough to use a CPU, but for production use, you should use a GPU.

An example of a retriever config is the following:

```json
{
    ""name"": ""e5-retriever"",
    ""model_name"": ""intfloat/e5-small-v2"",
    ""batch_size"": 1,
    ""device"": ""cpu""
}
```",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#retriever-config
modules > ia-configuration > the-rag-pipeline > llm-config,"modules > ia-configuration > the-rag-pipeline > llm-config: The LLM is the component that defines the model that will generate the answer from the prompt.

The LLM is configured with the following properties:

- **name**: Just a name for the LLM.
- **llm_type**: The type of LLM to use. It can be 'OpenAI', 'Local GPU Model' (HuggingFace), 'Local CPU Model (ggml)' or a 'vLLM Client'. Default: 'Local GPU Model'.
- **llm_name**: The name of the LLM to use. It can be a HuggingFace repo id, an OpenAI model id, etc. Default: gpt2.
- **ggml_llm_filename**: The GGML filename of the model, if it is a GGML model.
- **model_config**: The huggingface model config of the model, needed for GGML models.
- **load_in_8bit**: Whether to load the model in 8bit or not, only for Local GPU HuggingFace models. Default: False.
- **use_fast_tokenizer**: Whether to use the fast tokenizer or not. Default: True.
- **trust_remote_code_tokenizer**: Whether to trust the remote code for the tokenizer or not. Default: False.
- **trust_remote_code_model**: Whether to trust the remote code for the model or not. Default: False.
- **revision**: The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a git-based system for storing models. Default: main.
- **model_max_length**: The maximum length of the model. Default: None.


Our preferred option is to use an open-source LLM like [Llama-2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) for English and [Qwen-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat) for other languages.

To access Llama-2 models you need to set a environment variable in the back `.env` file with your HuggingFace API key:
```
HUGGINGFACE_KEY=XXXXXX
```


We can run these models locally, using a GPU or a CPU. ",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#llm-config
modules > ia-configuration > the-rag-pipeline > llm-config > gpu,"modules > ia-configuration > the-rag-pipeline > llm-config > gpu: For GPU we recommend using the following configuration:
```json
{
    ""name"": ""Llama2_GPU"",
    ""llm_type"": ""Local GPU Model"",
    ""llm_name"": ""meta-llama/Llama-2-7b-chat-hf"",
    ""revision"": ""main"",
    ""load_in_8bit"": false,
    ""use_fast_tokenizer"": true,
    ""trust_remote_code_tokenizer"": false,
    ""trust_remote_code_model"": false,
    ""model_max_length"": 4096
}
```
This uses the HuggingFace model implementations.

> ⚠️ To know if our GPU is enough to load the model we need to multiply its number of parameters by 2. For example, Llama-2-7B has 7B parameters, so we need at least 14GB of GPU memory to load it. This is because every parameter is stored in 2 bytes.",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#gpu
modules > ia-configuration > the-rag-pipeline > llm-config > cpu,"modules > ia-configuration > the-rag-pipeline > llm-config > cpu: For CPU we recommend using the following configuration:
```json
{
    ""name"": ""Llama2_CPU"",
    ""llm_type"": ""Local CPU Model (ggml)"",
    ""llm_name"": ""TheBloke/Llama-2-7B-GGML"",
    ""revision"": ""main"",
    ""ggml_llm_filename"": ""llama-2-7b.ggmlv3.q4_0.bin"",
    ""model_config"": ""meta-llama/Llama-2-7b-chat-hf"",
    ""use_fast_tokenizer"": true,
    ""trust_remote_code_tokenizer"": false,
    ""trust_remote_code_model"": false,
    ""model_max_length"": 4096
}
```

This uses the [GGML](https://github.com/ggerganov/ggml/) library and [CTransformers](https://github.com/marella/ctransformers/tree/main) for python bindings. For a list of available models, check [here](https://github.com/marella/ctransformers/tree/main#supported-models).

For these configurations we need to specify the repo where the models files are stored (llm_name) and then the filename of the model file (ggml_llm_filename). We also need to specify the model config, which is the HuggingFace model config of the model.

> ⚠️ To know if our CPU is enough to run the model we need to divide its number of parameters by 2. For example, Llama-2-7B has 7B parameters, so we need at least 3.5GB of RAM to run it. This is because it uses 4 bit quantization and every parameter is stored in 4 bits.
>
> Given that our prompts are long, the time to get the first word can be long (several seconds), but after that the generation is fast.",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#cpu
modules > ia-configuration > the-rag-pipeline > llm-config > openai,"modules > ia-configuration > the-rag-pipeline > llm-config > openai: This is the easiest way to get a model running. We just need to specify the model type and the model name. For example:
```json
{
    ""name"": ""ChatGPT"",
    ""llm_type"": ""OpenAI"",
    ""llm_name"": ""gpt-3.5-turbo""
}
```

The OpenAI models are specified [here](https://platform.openai.com/docs/models/). `gpt-3.5-turbo` should be enough for most use cases. To access OpenAI models you need to set a environment variable in the back `.env` file with your OpenAI API key:
```
OPENAI_API_KEY=XXXXXX
```",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#openai
modules > ia-configuration > the-rag-pipeline > llm-config > vllm-client,"modules > ia-configuration > the-rag-pipeline > llm-config > vllm-client: This uses a client to connect to a [vLLM server](https://github.com/vllm-project/vllm). The vLLM server is a server that runs a LLM model and exposes an API to generate answers, it has the best latency and throughput performance.

To configure this server you need to:

- You need to specify the model that you want to use inside this [`Dockerfile`](https://github.com/It/It/tree/develop/model_engines/llm), and then follow [this instructions](https://github.com/It/It/tree/develop/model_engines/llm).
- - You need to specify the URL of the vLLM server in the `.env`. Usually it will be `VLLM_ENDPOINT_URL=http://localhost:5000/generate`.
- Start the back, go to the admin and from it you only need to specify that you want to use the `vLLM Client`.",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#vllm-client
modules > ia-configuration > the-rag-pipeline > prompt-config,"modules > ia-configuration > the-rag-pipeline > prompt-config: The prompt is the input that the LLM will use to generate the answer. This config indicates how to build the final prompt that the LLM reads.

- **name**: Just a name for this prompt.
- **system_prompt**: This system prompt indicates the LLM how to behave.
- **n_contexts_to_use**: The number of contexts from the Retriever to use in the generation process. Default: 3

We recommend using the [ChatML guidelines](https://github.com/openai/openai-python/blob/main/chatml.md), an of course using LLMs trained using this format, like Llama-2. The example here is the prompt format used by [Llama-2](https://huggingface.co/blog/llama2#how-to-prompt-llama-2):

In the system prefix you can use the following text behavior:
```
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
```

Then for the tags it is recommended to use the following:
```json
{
    ""name"": ""Llama2_PromptConfig"",
    ""system_prompt"": ""You are a helpful, respectful and honest assistant. <Rest of the text> If you don't know the answer to a question, please don't share false information.\n\n""
}
```

When constructing the prompt we just concatenate the tags in the following order:
1. system_tag
2. system_prompt
3. system_end
4. user_tag
5. user message
6. user_end
7. assistant_tag
8. assistant output
9. assistant_end

For **n_contexts_to_use** a standard practice is to use 3.

> ⚠️ If you use an OpenAI model you only need to specify the **system prefix**, the other fields are not used.
",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#prompt-config
modules > ia-configuration > the-rag-pipeline > generation-config,"modules > ia-configuration > the-rag-pipeline > generation-config: The generation config is used to define the characteristics of the second part from the RAG pipeline, the generation process. We use sampling to generate the answer.

The sampling generation process is configured with the following properties:

- **name**: Just a name for the generation process.
- **top_k**: The number of tokens to consider for the top-k sampling. Default: 50.
- **top_p**: The cumulative probability for the top-p sampling. Default: 1.0.
- **temperature**: The temperature for the sampling. Default: 0.2.
- **repetition_penalty**: The repetition penalty for the sampling. Default: 1.0.
- **seed**: The seed for the sampling. Default: 42.
- **max_new_tokens**: The maximum number of new tokens to generate. Default: 256.

We recommend setting the temperature to low values, less than 1.0 because we want the model to be factual, not creative. A very good guide of all this parameters can be found in the [HuggingFace documentation](https://huggingface.co/blog/how-to-generate).

An example of a generation config is the following:
```json
{
    ""name"": ""Llama2_GenerationConfig"",
    ""top_k"": 50,
    ""top_p"": 1.0,
    ""temperature"": 0.2,
    ""repetition_penalty"": 1.0,
    ""seed"": 42,
    ""max_new_tokens"": 256
}
```",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#generation-config
modules > ia-configuration > the-rag-pipeline > rag-config,"modules > ia-configuration > the-rag-pipeline > rag-config: Finally, the RAG config is used to glue all the previous components together.

It relates the different elements to create a RAG (Retrieval Augmented Generation) pipeline.

The RAG config is configured with the following properties:

- name: Just a name for the RAG config.
- knowledge_base: The knowledge base to use.
- llm_config: The LLM config to use.
- prompt_config: The prompt config to use.
- generation_config: The generation config to use.
- retriever_config: The retriever config to use.

Remember that currently all the relevant data/models can be accessed and modified from the Django admin panel ([http://localhost/back/admin/](http://localhost/back/admin/)) or from the CLI.


An example of a RAG config is the following:
```json
{
    ""name"": ""chatfaq_llama_rag"",
    ""knowledge_base"": ""It_KB"",
    ""llm_config"": ""Llama2_GPU"",
    ""prompt_config"": ""Llama2_PromptConfig"",
    ""generation_config"": ""Llama2_GenerationConfig"",
    ""retriever_config"": ""e5-retriever""
}
```",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#rag-config
modules > ia-configuration > using-your-rag-pipeline,"modules > ia-configuration > using-your-rag-pipeline: To create the RAG pipeline you just need to link all the components together. You can do it from the Django admin panel ([http://localhost/back/admin/](http://localhost/back/admin/)).

Then, if you go to the Celery logs you will see that the RAG pipeline is being built. This process can take several minutes, depending on the size of the knowledge base. When it is finished you will see a message like this:
```
[2023-10-20 11:03:22,743: INFO/MainProcess] Loading RAG config: chatfaq_llama_rag with llm: meta-llama/Llama-2-7b-chat-hf with llm type: Local GPU Model with knowledge base: chatfaq retriever: intfloat/e5-small-v2 and retriever device: cpu
```


Once you have created your RAG pipeline, you can use it to generate answers.

The last step will be to reference the name of the Rag Config from a state of your SDK's FSM. <a href=""/en/latest/modules/sdk/index.html#model-example"">Here is an example of that</a>",https://chatfaq.readthedocs.io/en/latest/modules/configuration/index.html#using-your-rag-pipeline
modules > interfaces > init,"modules > interfaces > init: It provides up to 3 interfaces to interact with the backend's models/data:

- [It CLI](./cli/index.md)
- [It Admin](./django-admin/index.md)
- [Django Admin](./chatfaq-admin/index.md)",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/index.html
modules > interfaces > cli > init,"modules > interfaces > cli > init: It Command Line Interface (CLI) Tool

The CLI allows you to interact with the backend server from the command line. It offers the same capabilities as the Django admin panel or the It admin.",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html
modules > interfaces > cli > prerequisites,"modules > interfaces > cli > prerequisites: Make sure the next list of packages are installed on your system:

- Python 3.10
- poetry/pip",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#prerequisites
modules > interfaces > cli > installation > local-build,modules > interfaces > cli > installation > local-build: `poetry install`,https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#local-build
modules > interfaces > cli > installation > pypi,modules > interfaces > cli > installation > pypi: `pip install chatfaq-cli`,https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#pypi
modules > interfaces > cli > usage,"modules > interfaces > cli > usage: First of all you should configure the remote target server:

`chatfaq config host <REMOTE_ADRESS>`

Then you can log in into the remote back-end server:

`chatfaq config auth <TOKEN>`

You can retrieve the auth token from the backend server:

`curl -X POST -u username:password http://localhost:8000/back/api/login/`

by using an admin's user and password.",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#usage
modules > interfaces > cli > commands,"modules > interfaces > cli > commands: For a full list of commands and options run:

`chatfaq --help`",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#commands
modules > interfaces > cli > notes,"modules > interfaces > cli > notes: For the autocompletion to work you should run:

`chatfaq --install-completion`

or run `chatfaq --show-completion` and add the output to your shell's configuration file.",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/cli/index.html#notes
modules > interfaces > chatfaq-admin > init,"modules > interfaces > chatfaq-admin > init: The It admin will be a node application that allows you to manage your chatbot's relevant data from the browser.

It is currently under development.",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/chatfaq-admin/index.html
modules > interfaces > django-admin > init,"modules > interfaces > django-admin > init: The Django admin is an automatic admin interface that comes with Django. It allows you to manage your chatbot's relevant data from the browser.

You can access it with an <a href=""/en/latest/modules/installations/index.html#set-up"">admin account</a> at [http://localhost/back/admin/](http://localhost/back/admin/).",https://chatfaq.readthedocs.io/en/latest/modules/interfaces/django-admin/index.html
