from logging import getLogger
import asyncio

import ray
from ray.util.placement_group import (
    placement_group,
    remove_placement_group,
)
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

from back.apps.language_model.models.enums import (
    DeviceChoices,
    RetrieverTypeChoices,
)

logger = getLogger(__name__)


@ray.remote(num_cpus=1, resources={"tasks": 1})
def generate_titles_task(knowledge_base_pk, llm_config_id, n_titles=10):
    """
    Generate titles for the knowledge items of a knowledge base.
    Parameters
    ----------
    knowledge_base_pk : int
        The primary key of the knowledge base.
    llm_config_id : int
        The LLM to use for generating the titles.
    n_titles : int
        The number of titles to generate for each knowledge item. Not used for now.
    """

    from back.apps.language_model.models import (
        KnowledgeBase,
        KnowledgeItem,
        AutoGeneratedTitle,
        LLMConfig
    )
    from chat_rag.llms import load_llm
    from chat_rag.utils.gen_question import agenerate_questions

    kb = KnowledgeBase.objects.get(pk=knowledge_base_pk)
    k_items = KnowledgeItem.objects.filter(knowledge_base=knowledge_base_pk)

    llm_config = LLMConfig.objects.get(pk=llm_config_id)

    llm = load_llm(
        llm_config.llm_type,
        llm_config.llm_name,
        base_url=llm_config.base_url,
        model_max_length=llm_config.model_max_length,
    )

    questions = asyncio.run(
        agenerate_questions(
            [f"{item.title} {item.content}" for item in k_items],
            llm,
        )
    )
    
    auto_gen_questions = []
    for question, item in zip(questions, k_items):
        auto_gen_questions.append(
            AutoGeneratedTitle(
                knowledge_item=item,
                title=question,
            )
        )
    AutoGeneratedTitle.objects.bulk_create(auto_gen_questions)

    print(f"Questions generated for knowledge base: {kb.name}")


@ray.remote(num_cpus=1, resources={"tasks": 1})
def generate_intents(clusters_texts, llm_config_id):
    from chat_rag.intent_detection import agenerate_intents
    from back.apps.language_model.models import LLMConfig
    from chat_rag.llms import load_llm

    llm_config = LLMConfig.objects.get(pk=llm_config_id)
    llm = load_llm(
                llm_config.llm_type,
                llm_config.llm_name,
                base_url=llm_config.base_url,
                model_max_length=llm_config.model_max_length,
            )

    print("Generating intents...")
    intents = asyncio.run(agenerate_intents(clusters_texts, llm))
    return intents


@ray.remote(num_cpus=1, resources={"tasks": 1}, num_returns=2)
def get_similarity_scores(titles, kb_id):
    def retrieve(queries, kb_id, top_k=1):
        from back.apps.language_model.models import KnowledgeItem, KnowledgeBase, RAGConfig
        from chat_rag.retrievers import ColBERTRetriever, SemanticRetriever, ReRankRetriever
        from chat_rag.embedding_models import E5Model

        items = KnowledgeItem.objects.filter(knowledge_base=kb_id)
        items = [f"{item.title} {item.content}" for item in items]
        lang = KnowledgeBase.objects.get(pk=kb_id).lang

        rag_configs = RAGConfig.objects.filter(knowledge_base=kb_id)

        # We check if there is a RAGConfig with the ColBERT retriever
        # If not we default to a E5Model because the multilingual versions of ColBERT are too
        # large to run without knowing exactly the hardware resources
        rag_config = None
        for config in rag_configs:
            if config.retriever_config.get_retriever_type() == RetrieverTypeChoices.COLBERT:
                rag_config = config
                break
        if rag_config:
            retriever = ColBERTRetriever(
                model_name=rag_config.retriever_config.model_name,
            )
            retriever.index(
                items, use_faiss=False
            )
        else:
            model_name = 'intfloat/e5-small-v2' if lang == 'en' else 'intfloat/multilingual-e5-small'
            e5_model = E5Model(model_name=model_name)
            embeddings = e5_model.build_embeddings(items)
            data = {
                'contents': items,

            }
            retriever = SemanticRetriever(data=data, embeddings=embeddings, embedding_model=e5_model)
        
        rerank_retriever = ReRankRetriever(retriever, lang, 'cpu')

        results = rerank_retriever.retrieve(queries, top_k=top_k)
        
        return results
        

    import numpy as np

    results = retrieve(titles, kb_id)

    similarities = [item[0]["score"] for item in results]
    mean_similarity = np.mean(similarities)
    std_similarity = np.std(similarities)

    return mean_similarity, std_similarity


@ray.remote(num_cpus=1.0, resources={"tasks": 1.0})
def clusterize_texts_task(texts, batch_size, lang, device, low_resource):
    from chat_rag.intent_detection import clusterize_text

    labels = clusterize_text(
        texts=texts,
        batch_size=batch_size,
        lang=lang,
        device=device,
        low_resource=low_resource,
    )
    return labels


@ray.remote(num_cpus=0.5, resources={"tasks": 1})
def generate_suggested_intents_task(knowledge_base_pk, batch_size: int = 32, _generate_titles=False, low_resource=True):
    """
    Generate new intents from the users' queries. Orchestrator task that calls the other tasks.
    Parameters
    ----------
    knowledge_base_pk : int
        The primary key of the knowledge base.
    batch_size : int, optional
        Batch size for the embedding model, by default 32.
    _generate_titles : bool, optional
        If True, generates titles for the knowledge items, by default False.
    """

    from django.db.models import Max
    from django.db.models import Subquery, OuterRef

    from back.apps.language_model.prompt_templates import get_queries_out_of_domain
    from back.apps.language_model.models import (
        RAGConfig,
        AutoGeneratedTitle,
        Intent,
    )
    from back.apps.broker.models.message import Message

    rag_conf = RAGConfig.objects.filter(knowledge_base=knowledge_base_pk).first()
    if not rag_conf:
        logger.error(f"No RAG config found for knowledge base: {knowledge_base_pk}, please create one first so the intents can be generated.")
        return

    if _generate_titles:
        task_name = f"generate_titles_{knowledge_base_pk}"
        # block until the task is finished
        ray.get(generate_titles_task.options(name=task_name).remote(knowledge_base_pk, rag_conf.llm_config.pk))


    # These are in domain titles
    titles_in_domain = AutoGeneratedTitle.objects.filter(
        knowledge_item__knowledge_base=knowledge_base_pk
    )[:100]

    # Get the RAG config that corresponds to the knowledge base
    rag_conf = RAGConfig.objects.filter(knowledge_base=knowledge_base_pk).first()
    if not rag_conf:
        print(f"No RAG config found for knowledge base: {knowledge_base_pk}")
        return
    lang = rag_conf.knowledge_base.get_lang().value

    # if the retriever type is not e5, then return
    if rag_conf.retriever_config.get_retriever_type() != RetrieverTypeChoices.E5:
        print(
            f"Intent generation is not supported for retriever type: {rag_conf.retriever_config.get_retriever_type().value} right now"
        )
        return

    task_name = f"get_similarity_scores_{knowledge_base_pk}_in_domain"
    titles_in_domain_str = [title.title for title in titles_in_domain]
    in_domain_task_ref = get_similarity_scores.options(name=task_name).remote(
        titles_in_domain_str,
        knowledge_base_pk,
    )

    task_name = f"get_similarity_scores_{knowledge_base_pk}_out_domain"
    title_out_domain = get_queries_out_of_domain(lang)
    out_domain_task_ref = get_similarity_scores.options(name=task_name).remote(
        title_out_domain,
        knowledge_base_pk,
    )

    mean_sim_in_domain, std_sim_in_domain = ray.get(in_domain_task_ref)
    mean_sim_out_domain, std_sim_out_domain = ray.get(out_domain_task_ref)

    print(f"Mean similarity in domain: {mean_sim_in_domain}, std: {std_sim_in_domain}")
    print(
        f"Mean similarity out domain: {mean_sim_out_domain}, std: {std_sim_out_domain}"
    )

    # The suggested new intents will have a similarity score between the in domain queries and the out of domain queries
    new_intents_thresholds = {
        "max": mean_sim_in_domain - std_sim_in_domain,
        "min": mean_sim_out_domain + std_sim_out_domain,
    }

    print(f"Suggested intents thresholds: {new_intents_thresholds}")

    # check that the max is greater than the min
    if new_intents_thresholds["max"] < new_intents_thresholds["min"]:
        print(
            "Max threshold is lower than min threshold, no new intents will be generated"
        )
        return

    subquery = (
        Message.objects.filter(
            stack__contains=[
                {
                    "payload": {"references": {"knowledge_base_id": "1"}},
                    "type": "lm_generated_text",
                }
            ]
        )
        .annotate(
            stack_element=Subquery(
                Message.objects.filter(id=OuterRef("id")).values("stack")[:1]
            )
        )
        .values("prev_id")
    )

    # Main query to get the messages where id is in the subquery result
    messages = Message.objects.filter(id__in=Subquery(subquery), sender__type="human")

    print(f"Number of messages: {messages.count()}")

    # filter the results if the max similarity is between the thresholds
    messages = messages.filter(
        max_similarity__lte=new_intents_thresholds["max"],
        max_similarity__gte=new_intents_thresholds["min"],
    )

    print(f"Number of messages after filtering: {messages.count()}")

    if messages.count() == 0:
        print("There are no suggested intents to generate")
        return

    messages_text = [
        Message.objects.get(id=item["message_id"]).stack[0]["payload"]
        for item in messages
    ]

    print("Clusterizing texts...")
    task_name = f"clusterize_texts_{knowledge_base_pk}"
    # We try to place the task on a GPU if available
    if low_resource:
        # Use only CPU resources when low_resource is True
        print('Adquiring a CPU for the low resource clustering task...')
        pg = placement_group([{"CPU": 1.0, "tasks": 1.0}], strategy="STRICT_PACK", name="clusterize_texts")
        device = 'cpu'
    else:
        # Try to use GPU when low_resource is False
        print('Adquiring a GPU for the clustering task...')
        pg = placement_group([{"CPU": 1.0, "GPU": 1.0, "tasks": 1.0}], strategy="STRICT_PACK", name="clusterize_texts")
        device = 'cpu'

    try:
        ray.get(pg.ready(), timeout=10)
        if not low_resource: # assing GPU
            device = 'gpu'
        scheduling_strategy = PlacementGroupSchedulingStrategy(
            placement_group=pg,
            placement_group_bundle_index=0,
        )
    except Exception:
        if not low_resource:
            print("There are no GPUs available or placement group creation failed, using CPUs for the task")
        scheduling_strategy = None

    labels = ray.get(
        clusterize_texts_task.options(
            name=task_name,
            scheduling_strategy=scheduling_strategy,
        ).remote(messages_text, batch_size, lang, device, low_resource)
    )

    k_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    print(f"Number of clusters: {k_clusters}")

    # list of lists of queries associated to each cluster
    clusters = [[] for _ in range(k_clusters)]
    cluster_instances = [[] for _ in range(k_clusters)]
    for label, query, message_instace in zip(labels, messages_text, messages):
        if label != -1:  # -1 is the label for outliers
            clusters[label].append(query)
            cluster_instances[label].append(message_instace)

    # generate the intents
    task_name = f"generate_intents_{knowledge_base_pk}"
    intents = ray.get(generate_intents.options(name=task_name).remote(clusters, rag_conf.llm_config.pk))

    # save the intents
    new_intents = [
        Intent(
            intent_name=intent,
            auto_generated=True,
            valid=False,
            suggested_intent=True,
        )
        for intent in intents
    ]

    Intent.objects.bulk_create(new_intents)

    print(f"Number of new intents: {len(new_intents)}")

    # add the messages to each intent
    for intent_cluster, intent in zip(cluster_instances, new_intents):
        # get the value of key 'message_id' from each message
        intent_cluster = [item["message_id"] for item in intent_cluster]
        intent.message.add(*intent_cluster)

    print("New intents generated successfully")


@ray.remote(num_cpus=0.5, resources={"tasks": 1})
def generate_intents_task(
    knowledge_base_pk, batch_size: int = 32, low_resource: bool = True
):
    """
    Generate existing intents from a knowledge base. Orchestrator task that calls the other tasks.
    Parameters
    ----------
    knowledge_base_pk : int
        The primary key of the knowledge base.
    batch_size : int, optional
        Batch size for the embedding model, by default 32.
    low_resource : bool, optional
        If True, uses TF-IDF for vectorization, by default False.
    """

    from back.apps.language_model.models import (
        Intent,
        RAGConfig,
        KnowledgeItem,
        KnowledgeBase,
    )

    rag_conf = RAGConfig.objects.filter(knowledge_base=knowledge_base_pk).first()
    if not rag_conf:
        logger.error(f"No RAG config found for knowledge base: {knowledge_base_pk}, please create one first so the intents can be generated.")
        return
    

    kb = KnowledgeBase.objects.get(pk=knowledge_base_pk)
    lang = kb.get_lang().value

    k_items = KnowledgeItem.objects.filter(knowledge_base=knowledge_base_pk)

    # the texts are a concatenation of the title and the content
    texts = [
        f"{item.title} {item.content}" for item in k_items
    ]  # concatenate the title and the content

    print(f"Generating intents for {k_items.count()} knowledge items")
    
    print("Clusterizing texts...")
    task_name = f"clusterize_texts_{knowledge_base_pk}"
    # We try to place the task on a GPU if available
    if low_resource:
        # Use only CPU resources when low_resource is True
        print('Adquiring a CPU for the low resource clustering task...')
        pg = placement_group([{"CPU": 1.0, "tasks": 1.0}], strategy="STRICT_PACK", name="clusterize_texts")
        device = 'cpu'
    else:
        # Try to use GPU when low_resource is False
        print('Adquiring a GPU for the clustering task...')
        pg = placement_group([{"CPU": 1.0, "GPU": 1.0, "tasks": 1.0}], strategy="STRICT_PACK", name="clusterize_texts")
        device = 'cpu'

    try:
        ray.get(pg.ready(), timeout=10)
        if not low_resource: # assing GPU
            device = 'gpu'
        scheduling_strategy = PlacementGroupSchedulingStrategy(
            placement_group=pg,
            placement_group_bundle_index=0,
        )
    except Exception:
        if not low_resource:
            print("There are no GPUs available or placement group creation failed, using CPUs for the task")
        scheduling_strategy = None

    labels = ray.get(
        clusterize_texts_task.options(
            name=task_name,
            scheduling_strategy=scheduling_strategy,
        ).remote(texts, batch_size, lang, device, low_resource)
    )

    # Now we remove the pg because we don't want to keep it alive
    remove_placement_group(pg)

    k_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    print(f"Number of clusters: {k_clusters}")

    # dict of lists of texts associated to each cluster
    clusters = {}
    cluster_k_item_instances = {}
    for label, text, k_item  in zip(labels, texts, k_items):
        if label != -1:  # -1 is the label for outliers
            if label not in clusters:
                clusters[label] = []
                cluster_k_item_instances[label] = []
            clusters[label].append(text)
            cluster_k_item_instances[label].append(k_item)

    # generate the intents
    print("Generating intents...")
    task_name = f"generate_intents_{knowledge_base_pk}"
    intents = ray.get(generate_intents.options(name=task_name).remote(clusters, rag_conf.llm_config.pk))

    print(f"Number of new intents: {len(intents)} generated")

    # save the intents
    new_intents = [
        Intent(
            intent_name=intent,
            auto_generated=True,
            valid=False,
            suggested_intent=False,
        )
        for intent in intents
    ]

    Intent.objects.bulk_create(new_intents)

    print("Suggested intents saved successfully")

    # add the knowledge items to each intent
    for (_, items), intent in zip(cluster_k_item_instances.items(), new_intents):
        intent.knowledge_item.add(*items)

    print("Knowledge items added to the intents successfully")
